{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN  - This is a Multilayer Perceptron (MLP) example using Keras\n",
    "\n",
    "    Copyright (C) 2020 Adrian Bevan, Queen Mary University of London\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "    \n",
    "----------------------\n",
    "\n",
    "## Useful background on MNIST and data wrangling\n",
    "\n",
    "MLP example using a Keras model. This is a simple example of a 2 layer MLP processing the MNIST data using the Adam optimiser.  The MNIST data is a set of 60,000 training and 10,000 test examples.  Each example is a hand written integer between 0 and 9, represented by a greyscale image with 28x28 pixels. This constitutes a 784 dimensional input feature space, where each image is a number between 0 and 255 (8 bit greyscale).  In order for an image to be processed efficiently by a neural network the pixel colour in the range $[0, 255]$ is mapped into the domain $[0, 1]$, following the usual [Efficient Backpropagation](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) data wrangling guidelines. For an MLP the 28x28 pixel image features (i.e. the pixels) are considered a linear set of 784 features that are input to the network.\n",
    "\n",
    "For more information about the MNIST data set please see the website http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "For more information about the Adam optimiser please see the  paper by Kingma and Ba, [arXiv:1412.6980](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "----------------------\n",
    "## Load and pre-process the data\n",
    "\n",
    "The MNIST data are directly accessible via keras as a dataset. So we first load the data. As noted above (and as you will have explored in the linear regression example), it is important to closely match the weights used in a network with the feature space domain that is being studied, so that the optimiser has less work to do in order to converge to the optimal solution.  In this case we achieve that by mapping the 8-bit greyscale color value $[0, 255]$ on the domain $[0, 1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mLoading the MNIST data from keras\u001b[0m\n",
      "\tN(test) =  60000\n",
      "\tN(test) =  10000\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\033[92mLoading the MNIST data from keras\\033[0m\")\n",
    "# Load the MNIST data via the tensorflow keras dataset:\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(\"\\tN(test) = \", len(x_train))\n",
    "print(\"\\tN(test) = \", len(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Building the model \n",
    "\n",
    "**Training [Epochs, batches and validation data]** The number of training epochs specified is denoted by `Nepochs`.  1 epoch is required to run over all of the training data.  It is possible to run batches or mini-batches of data through the training; each batch requires the optimisation to be performed, and so when specifying the `BatchSize` the training will be performed by running the optimisation $N_{epochs}\\times N_{batches}$ times.  In general this leads to faster optimisation of the model than optimising over the full training set each time.\n",
    "\n",
    "The training data will be split into training and validation samples according to the value of the variable `ValidationSplit`. \n",
    "\n",
    "**Dropout:** Coadaptation is the ability for an optimisation algorithm to allow weights to be learned where changes in one node can be compensated by changes in another node that limit the increase in performance.  This issue can be a problem for deep networks in particular where the optimisation process can involve millions of hyperparameters.  A way to combat this issue is to randomly drop-out nodes in the network each iteration of the optimisation.  That way no single paring of nodes can learn to co-adapt to the evolution of hyperparameters through the optimisation.  Thankfully all the user has to do is to set a dropout value via the variable `DropoutValue`. \n",
    "\n",
    "**NOTE:** this value is the fraction of nodes dropped from the model [In V1.X of TensorFlow the opposite convention was used].\n",
    "\n",
    "**Loss:** The cross entropy loss function is used for this optimisation process.  The value of the loss function is converted into an output vector of 1's and 0's to be used for classification.\n",
    "\n",
    "### Model configuration\n",
    "\n",
    "The MLP consists of an input layer, two hidden layers, in this case one drop out layer and finally an output layer. These are as follows:\n",
    "- **Inputs:** This model specifies an input shape of 28x28 that is flattened. The purpose of this is to ensure that there are 784 input features being fed into the hidden layers of the network.\n",
    "- **Hidden Layers:** This network has two hidden layers, wich using a leaky ReLU (Rectified Linear Unit) activation function.  The parameter alpha defines the slope of the function for negative values, and for positive values the activation function is simply $f(x)=x$.\n",
    "- **Dropout Layer:** Here the only dropout layer is the one from the second hidden layer of the network to the output.  You may wish to explore what happens if another dropout layer is added after the first hidden layer.\n",
    "- **Output:** The output is a vector of 10 numbers. As the loss function is a cross entropy loss function the output in this case will be a one hot vector, i.e. a vector of 10 digits that are either 0 or 1.  The element corresponding to 1 being the optimal classificaiton of the example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mWill train a multilayer perceptron on MNIST data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Input data MNIST\n",
      "2 layer MLP with configuration 784:128:128:10\n",
      "Dropout values       =  0.6\n",
      "Leaky relu parameter =  0.1\n",
      "ValidationSplit      =  0.5\n",
      "BatchSize            =  20\n",
      "Nepochs              =  20 \n",
      "\n",
      "N(train)             =  60000\n",
      "N(test)              =  10000\n",
      "Train on 30000 samples, validate on 30000 samples\n",
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 3s 92us/sample - loss: 0.4178 - acc: 0.8766 - val_loss: 0.2351 - val_acc: 0.9273\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 3s 84us/sample - loss: 0.1898 - acc: 0.9456 - val_loss: 0.1512 - val_acc: 0.9539\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 2s 82us/sample - loss: 0.1404 - acc: 0.9584 - val_loss: 0.1272 - val_acc: 0.9616\n",
      "Epoch 4/20\n",
      "30000/30000 [==============================] - 3s 86us/sample - loss: 0.1094 - acc: 0.9657 - val_loss: 0.1379 - val_acc: 0.9610\n",
      "Epoch 5/20\n",
      "30000/30000 [==============================] - 2s 83us/sample - loss: 0.0951 - acc: 0.9716 - val_loss: 0.1163 - val_acc: 0.9665\n",
      "Epoch 6/20\n",
      "30000/30000 [==============================] - 2s 83us/sample - loss: 0.0820 - acc: 0.9756 - val_loss: 0.1434 - val_acc: 0.9602\n",
      "Epoch 7/20\n",
      "30000/30000 [==============================] - 3s 94us/sample - loss: 0.0697 - acc: 0.9779 - val_loss: 0.1466 - val_acc: 0.9638\n",
      "Epoch 8/20\n",
      "30000/30000 [==============================] - 3s 92us/sample - loss: 0.0601 - acc: 0.9814 - val_loss: 0.1533 - val_acc: 0.9633\n",
      "Epoch 9/20\n",
      "30000/30000 [==============================] - 3s 88us/sample - loss: 0.0610 - acc: 0.9804 - val_loss: 0.1353 - val_acc: 0.9678\n",
      "Epoch 10/20\n",
      "30000/30000 [==============================] - 3s 89us/sample - loss: 0.0514 - acc: 0.9839 - val_loss: 0.1335 - val_acc: 0.9685\n",
      "Epoch 11/20\n",
      "30000/30000 [==============================] - 3s 89us/sample - loss: 0.0483 - acc: 0.9845 - val_loss: 0.1472 - val_acc: 0.9684\n",
      "Epoch 12/20\n",
      "30000/30000 [==============================] - 3s 86us/sample - loss: 0.0443 - acc: 0.9862 - val_loss: 0.1570 - val_acc: 0.9680\n",
      "Epoch 13/20\n",
      "30000/30000 [==============================] - 3s 88us/sample - loss: 0.0426 - acc: 0.9860 - val_loss: 0.1500 - val_acc: 0.9696\n",
      "Epoch 14/20\n",
      "30000/30000 [==============================] - 3s 89us/sample - loss: 0.0399 - acc: 0.9869 - val_loss: 0.1544 - val_acc: 0.9693\n",
      "Epoch 15/20\n",
      "30000/30000 [==============================] - 3s 89us/sample - loss: 0.0404 - acc: 0.9869 - val_loss: 0.1671 - val_acc: 0.9683\n",
      "Epoch 16/20\n",
      "30000/30000 [==============================] - 3s 86us/sample - loss: 0.0317 - acc: 0.9894 - val_loss: 0.1556 - val_acc: 0.9712\n",
      "Epoch 17/20\n",
      "30000/30000 [==============================] - 3s 87us/sample - loss: 0.0363 - acc: 0.9889 - val_loss: 0.1876 - val_acc: 0.9670\n",
      "Epoch 18/20\n",
      "30000/30000 [==============================] - 3s 86us/sample - loss: 0.0350 - acc: 0.9888 - val_loss: 0.1973 - val_acc: 0.9677\n",
      "Epoch 19/20\n",
      "30000/30000 [==============================] - 3s 86us/sample - loss: 0.0332 - acc: 0.9896 - val_loss: 0.1805 - val_acc: 0.9699\n",
      "Epoch 20/20\n",
      "30000/30000 [==============================] - 3s 91us/sample - loss: 0.0307 - acc: 0.9908 - val_loss: 0.1825 - val_acc: 0.9695\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training configuration\n",
    "#\n",
    "ValidationSplit = 0.5\n",
    "BatchSize       = 20\n",
    "Nepochs         = 20\n",
    "DropoutValue    = 0.6\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),  # MNIST is 28x28 pixel images\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dropout(DropoutValue),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\033[92mWill train a multilayer perceptron on MNIST data\\033[0m\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Input data MNIST\")\n",
    "print(\"2 layer MLP with configuration 784:128:128:10\")\n",
    "print(\"Dropout values       = \", DropoutValue)\n",
    "print(\"Leaky relu parameter =  0.1\")\n",
    "print(\"ValidationSplit      = \", ValidationSplit)\n",
    "print(\"BatchSize            = \", BatchSize)\n",
    "print(\"Nepochs              = \", Nepochs, \"\\n\")\n",
    "print(\"N(train)             = \", len(x_train))\n",
    "print(\"N(test)              = \", len(x_test))\n",
    "\n",
    "# now specify the loss function - cross entropy\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# now we can train the model to make predictions.\n",
    "#   Use the ADAM optimiser\n",
    "#   Specify the metrics to report as accuracy\n",
    "#   Specify the loss function (see above)\n",
    "# the fit step specifies the number of training epochs\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "history  = model.fit(x_train, y_train, validation_split=ValidationSplit, batch_size=BatchSize, epochs=Nepochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Output\n",
    "\n",
    "When training a model we are interested in studying the accuracy of prediction (in this case how often do we correctly classify an MNIST image according to its true label), and about the evolution of the loss function for both the test and train samples of examples.  If the test and train sample loss functions are signficantly different that points toward a problem... the model could be under or over trained.\n",
    "\n",
    "This information is stored in the 'history' variable.  See the note regarding key value differences between TF1.0 and TF2.0 (here we use the TF2.0 convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTraining history contains the following keys:\u001b[0m\n",
      "\t loss\n",
      "\t acc\n",
      "\t val_loss\n",
      "\t val_acc\n",
      "\n",
      "\u001b[1mDisplay the evolution of the accuracy as a function of the training epoch\u001b[0m\n",
      "\n",
      "  N(Epochs)           =  20\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-916693911052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#plt.plot(history.history['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#plt.plot(history.history['val_acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  accuracy (train)     = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  accuracy (validate)  = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# Print out the history keys expected are:\n",
    "#    loss          The loss function evaluated at each epoch for the training set\n",
    "#    accuracy      The accuracy evaluated at each epoch for the training set\n",
    "#    val_loss      The loss evaluated at each epoch for the validation set\n",
    "#    val_accuracy  The accuracy evaluated at each epoch for the validation set\n",
    "# The val_* entries exist only if there is a validation_split specified\n",
    "#\n",
    "# NOTE: TFV1.0 uses acc and val_acc keys instead of accuracy and val_accuracy\n",
    "#\n",
    "keys = history.history.keys()\n",
    "print(\"\\033[1mTraining history contains the following keys:\\033[0m\")\n",
    "for key in keys:\n",
    "    print (\"\\t\", key)\n",
    "    \n",
    "print(\"\\n\\033[1mDisplay the evolution of the accuracy as a function of the training epoch\\033[0m\\n\")\n",
    "print(\"  N(Epochs)           = \", Nepochs)\n",
    "#print(\"  accuracy (train)    = \", history.history['acc'])\n",
    "#print(\"  accuracy (validate) = \", history.history['val_acc'])\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "print(\"  accuracy (train)     = \", history.history['accuracy'])\n",
    "print(\"  accuracy (validate)  = \", history.history['val_accuracy'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='lower right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "print(\"\\n\\033[1mDisplay the evolution of the loss as a function of the training epoch\\033[0m\\n\")\n",
    "print(\"  N(Epochs)        = \", Nepochs)\n",
    "print(\"  loss (train)     = \", history.history['loss'])\n",
    "print(\"  loss (validate)  = \", history.history['val_loss'])\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# having finished training the model, use this to evaluate the performance on a sample of test data\n",
    "print(\"\\n\\033[1mPerformance summary (on test data):\\033[0m\")\n",
    "loss, acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(\"\\tloss = {:5.3f}\\n\\taccuracy = {:5.3f}\".format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Suggested exercises:\n",
    "\n",
    " - Explore the effect of DropOut, ValidationSplit, Nepochs, and BatchSize have on the training (try to find a model where the test and train loss function values are similar.\n",
    " - Explore how the neural network structure affects the training performance (e.g. add double or halve the number of nodes in the hidden layers, the current value is 128 for both)\n",
    " - Explore the effect of adding a second dropout layer into the network after the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
